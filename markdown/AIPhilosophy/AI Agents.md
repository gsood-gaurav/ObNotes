Compound AI systems or AI agents are new research direction and and agent development is driven by benchmarks. Compound AI system is best way to maximize AI results in future and might be most impactful trends in AI in 2024. Over dozen agent benchmarks have been released, spanning domains such as web interaction, programming and tool use. Many benchmarks developed for LLM evaluation has been used for agent evaluation.

Agent evaluation differs from LLM evaluation in fundamental ways. Agents can be used for tasks that are harder more realistic and have more real world utility and usually don't have a single correct answer.

Agents can cost much more than a single model call.

Accuracy is measured by how often one of the top 10 answers generated by model is correct.

>[!note] WebArena is an agent benchmark that aims to evaluate agents on tasks on the web. It includes clones of six different websites(Gitlab, Reddit, Wikipedia, Openstreet maps, an ecommerce platoform and a content management system) and two tools (calculator and scratchpad). Top agent on the WebArena leaderboard is called **STeP**. 

Agent benchmarks don't account for human in the loop  which is another spectrum.
