Common convergence bounds in stochastic optimization improve with smaller gradient variance.
Mini-batch SGD is said to converge faster because the variance of the gradient estimates is reduced by a rate linear in the mini-batch size.
Variance in gradient estimates affect convergence and generalization

Gradient Variance increases during training.
Smaller learning rate coincides with higher variance.

