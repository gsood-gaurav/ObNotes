
Ì¥A diffusion model isn't that complex if you compare it to other [[Generative Modelling|generative models]] such as Normalizing Flows, GANs or [[Variational Autoencoder|VAEs]] **<span style="color:rgb(146, 208, 80)">they all convert noise from some simple distribution to a data sample.</span>**

Diffusion Model has both likelihood based and score based interpretations.

>[!note] Three predominant formulations of Diffusion Models
>- Denoising Diffusion Probabilistic Models (DDPMs)
>- Score Based Generative Models (SGMs)
>- Stochastic Differential Equations (SDE)

Key to all these approaches is to progressively perturb data with intensifying random noise (called diffusion process) then successively remove noise to generate new data samples.

Taxonomy of recent research that maps out the field of diffusion models, categorizing it into three key areas:
> [!success] Key areas 
>- efficient sampling 
>- improved likelihood estimation 
>- methods for handling data with special structures such as relational data, data with permutation/rotational invariance and data residing on manifolds.

## DDPM

- Makes use of two markov chains: a forward chain and backward chain
- Forward chain perturbs data to noise and reverse chain converts noise back to data
- Forward chain is hand designed with the goal to transform any data distribution into simple prior distribution (e.g. standard Gaussian)
- Backward chain reverses the former by learning transition kernels parameterized by deep neural networks
- New points are generated by first sampling a random vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain.
- Formally given a data distribution $x_0 \sim q(x_0)$, the forward process generates a sequence of random variables $x_1,x_2\cdots x_T$ with transition kernel $q(x_t|x_{t-1})$ 
- Using chain rule of probability we can factorize the joint distribution of $x_1, x_2\cdots x_T$  conditioned on $x_0$ denoted as $q(x_1,\cdots x_T|x_0)$ into 
$$
q(x_1,\cdots x_T|x_0) = \prod\limits_{i=1}^Tq(x_t|x_{t-1})
$$
- In DDPM we handcraft the kernel $q(x_t|x_{t-1})$ to incrementally transform the data distribution $q(x_0)$ into a tractable prior distribution
- One typical design for the transition kernel is Gaussian perturbation, and the most common choice for the transition kernel is 
$$
q(x_t|x_{t-1}) = \mathcal{N}(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t I )
$$
where $\beta_t\in(0,1)$ is a hyper parameter chosen ahead of model training. Other types of kernel are also applicable. Gaussian transition kernel allows to marginalize the joint distribution to obtain analytical form of $q(x_t|x_0)$ as follows

$$
q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\tilde{\alpha_t}}x_{0}, (1-\tilde{\alpha_t})I)
$$
- Given $x_0$ we can easily obtain a sample of $x_t$ by sampling a Gaussian vector $\epsilon \sim \mathcal{N}(0, I)$  and applying transformation 
$$
x_t = x_0\sqrt{\tilde{\alpha_t}} + \sqrt{1-\tilde{\alpha_t}}\epsilon
$$
where $\alpha_T \approx 0, x_T$ is almost Gaussian in distribution so we have $q(x_{T})=\int q(x_{T}|x_0)q(x_0)dx_0$   

- The reverse Markov chain is parameterized by a prior distribution $\mathcal{N}(x_T;0, I)$ and a learnable transition kernel $p_\theta(x_{t-1}|x_{t})$
- The transition kernel takes the form 
$$
p_\theta(x_{t-1}|x_{t}) = \mathcal{N}(x_{t-1};\mu_\theta(x_t,t), {\Sigma}_\theta(x_t, t))
$$

where $\mu_\theta(x_t,t), {\Sigma}_\theta(x_t, t)$ are parameterized by deep neural network.

- Key to success of this sampling process is training the reverse Markov chain to match the actual time reversal of the forward markov chain.
-  
The authors used Markov Chain to convert one distribution to another. They do this by adding noise in small steps to the original image till the resulting image just becomes noise. This step of adding noise is similar to how noise accumulates in Langevin Dynamics.

You can use this in two ways:

- If you compute the long-run average probabilities of being in various states from a dataset, these averages will be very close to those of the stationary distribution.
- Clearly, outcomes very close in time are correlated. However, when t is large enough, the probability of being in the state is independent of the staring point. Thus, if you sample from the chain every T periods, then the observations appear to be independent draws from a Multinomial Distribution whose probabilities are the same as the stationary distribution. In statistics, this fact is used in simulation estimation procedures called Markov Chain Monte Carlo (MCMC).

Both the forward and reverse process indexed by t happen for some number of finite time steps $T$ (the DDPM authors use $T=1000$). You start with $t=0$ where you sample a real image $x_0$ from your data distribution and the forward process samples noise from a Gaussian distribution at each time step $t$, which is added to the image of the previous time step. Given a sufficiently large $T$ and a well behaved schedule for adding noise at each time step, you end up with what is called an isotropic Gaussian distribution at $t=T$ via a gradual process.

## In more Mathematical Form

Let $q(x_0)$ be the real data distribution say of "real images". We sample from this distribution to get an image, $x_0 \sim q(x_0)$. We define the forward diffusion process $q(x_{t}|x_{t-1})$ which adds Gaussian noise at each time step $t$ according to a known variance schedule $0<\beta_1<\beta_2<...<\beta_T$ as 
$$
q(x_{t}|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)
$$
Recall that a normal distribution is defined by two parameters mean $\mu$  and variance $\sigma^2 \ge 0$. Basically, each new image at time step $t$ is drawn from a conditional Gaussian noise with $\mu_t = \sqrt{1-\beta_t}x_{t-1}$ and $\sigma^2=\beta_t$, which we can do by sampling $\epsilon \sim \mathcal{N}(0,I)$ and then setting $x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon$  or it can be written as $x_t = \sqrt{\alpha_{t}}x_{t-1}+\sqrt{1-\alpha_{t}}\epsilon$.

The VDM posterior can be written as follows:
$$
\begin{align*}
q(x_{1:T}|x_0) = \prod\limits_{t=1}^T q(x_t|x_{t-1})\tag{30}
\end{align*}
$$

From the second assumption we know the distribution of each latent variable in the encoder is a Gaussian centered around previous hierarchical latent. Unlike Markovian HVAE the structure of the encoder at each time step $t$ is not learned; it is fixed linear Gaussian Model where mean and standard deviation can be set as hyperparameters or learned as parameters. We parameterize the Gaussian encoder with mean $\mu(t) = \sqrt{\alpha_t}\ x_{t-1}$  and variance $\sum_t(x_t) = (1-\alpha_t)I$ where the form of coefficients are chosen in such  a way that the encoding process is variance-preserving i.e. variance of each latent variable stays at similar scale.

Joint distribution in case of VDM is given as
$$
\begin{align*}
p(x_{0:T}) &= p(x_T) \prod\limits_{t=1}^Tp_\theta(x_{t-1}|x_t) \\
where, \\
p{x_T} &= \mathcal{N}(x_T;0, I)
\end{align*}
$$
## Maximizing the ELBO

$$
\begin{align*}
log\ p(x_0) &= log\int p(x_0, x_1,...x_T) dx_{1...t}\tag{34} \\
&= log \int \frac{p(x_{0:T})\ q(x_{1:T}|x_0)\ dx_{1:T}}{q(x_{1:T}|x_0)}\tag{35} \\
&= log\ E_{q(x_{1:T}|x_0)}\left[\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]\tag{36} \\
&\ge E_{q(x_{1:T}|x_0)}\ log \left[\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}\right]\tag{37} \\
&= E_{q(x_{1:T}|x_0)}\ log \left[\frac{p(x_T)p(x_0|x_1)\prod\limits_{t=2}^T p(x_{t-1}|x_t)}{q(x_T|x_{T-1})\prod\limits_{t=1}^{T-1}q(x_{t}|x_{t-1})}\right]\tag{38}\\
&= E_{q(x_{1:T}|x_0)}\ log \left[\frac{p(x_T)p(x_0|x_1)\prod\limits_{t=1}^{T-1} p(x_{t}|x_{t+1})}{q(x_T|x_{T-1})\prod\limits_{t=1}^{T-1}q(x_{t}|x_{t-1})}\right]\tag{39}\\
&= E_{q(x_{1:T}|x_0)}\ log \left[\frac{p(x_T)p(x_0|x_1)}{q(x_T|x_{T-1})}\right] + E_{q(x_{1:T}|x_0)} log\left[\prod\limits_{t=1}^{T-1} \frac{p(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]\tag{40}\\
&= E_{q(x_{1}|x_0)} log\left[p(x_0|x_1)\right]
+ E_{q(x_{T-1}, x_{T}|x_0)} log\left[\frac{p(x_T)}{q(x_T|x_{T-1})}\right] 
+ E_{q(x_{T-1}, x_{T}, x_{T+1}|x_0)} log\left[\prod\limits_{t=1}^{T-1} \frac{p(x_{t}|x_{t+1})}{q(x_{t}|x_{t-1})}\right]\tag{41}\\
\end{align*}
$$
