Transformers are deep neural networks that replace CNNs and RNNs with self-attention. Self attention allows Transformers to easily transmit information across the input sequence.

>Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sequence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations or embeddings for each word.. Then, using self-attention, it aggregates information from all other words, generating a new representation per word informed by the entire context. This step is then repeated multiple times in parallel for all words, successively generating new representations![[Pasted image 20230417123613.png]]