In [[Statistical Hypothesis Testing]] a result is statisitically significant if it is very unlikely to have occured given the [[null hypothesis]]. More precisely, a study's significance level, denoted by $\alpha$, is the probability of study rejecting the null hypothesis, given the null hypothesis is true. The significance level for a study is chosen before data collection, and is typically set to $5\%$ or much lower depednding on the field of study. 

In any experiment or observation that invloves drawing a sample from the population, there is always the possibility that an observed effect would have occured due to sampling error alone. But if p-value of an observed effect is less than the significance level, an investigator may conclude that effect reflects the characterstics of the whole population, thereby rejecting the null hypothesis.

**A study found to be statistically significant may not be practically significant**.
Ronald Fisher in his book refers to a _statistical test_ that summarizes  the compatibility of data with a proposed model and produces a [[p-value]]. Fisher suggest that researchers might consider a pvalue of 0.05  as a handy guide. p-values are regulary misinterpreted and statistical significance is not the same thing as practical significance.

The use of p-values for nearly a century to determine statistical significance of experimental results has contributed to an illusion of certainity and reproducibilty crises in many scientific fields.

Statistical Significance should indicate a certain level of interest.

