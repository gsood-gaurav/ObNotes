Given observed samples $x$ from a distribution of interest, the goal of **generative model** is to learn to model its true data distribution $p(x)$. Once learned, we can generate new samples  from our approximate model at will.<span style="color:rgb(192, 0, 0)"> Furthermore under some formulations, we are able to use the learned model to evaluate the likelihood of observed or sampled data as well.</span>

## DLVM
There are various kinds of Generative Models 
- Implicit Models
	Provides a sampling mechanism for generating data but don't explicitly define likelihood function
	- Generative Adversarial Networks
		- Model the sampling procedure of a complex distribution which is learned in an adversarial manner
	- Stochastic Simulator Models
- Explicit Models (Likelihood Based)
	- Seeks to learn a model that assigns high likelihood to the observed samples.
	We have access to model likelihood function
	- [[Variational Autoencoder]]
	- [[Normalizing Flows]]
	- Autoregressive Models
	- [[Diffusion Models]] (has both likelihood based and score based interpretations)
- Energy Based Models
	- Distribution is learnt as a flexible energy function that is then normalized.
- Score based Models
	- Instead of learning the energy function itself, they learn score of the energy based model as a neural network.



> Abstract Representations/World Models
> 
> For many modalities we can think of data we observe as generated or represented by an associated unseen latent variable $z$. The best intuition for expressing this idea is through Plato's Allegory of the cave. In the allegory, a group of people are chained inside a cave their entire life and can only see the two-dimensional shadows projected  onto a wall in front of them, which are generated by unseen three dimensional objects passed before a fire. To such people, everything they observe is actually determined by higher-dimensional abstract concepts that they can never behold.
>Analogously the objects we may encounter in actual world may also be generated as a function of some higher level representations; for example such representations may encapsulate abstract properties such as color, size, shape and more. Then what we observe can be interpreted as a three dimensional projection or instantiation of of such abstract concepts, just as what the cave people observe is actually two-dimensional projection of three dimensional objects. Whereas cave people can never see the hidden objects, they can still reason or draw inferences about them, in a similar way, we can approximate latent representations that describe the data we observe. 
>While in Plato's allegory illustrates the idea behind latent variables as potentially unobservable representations that determine observations, a caveat of this analogy is that in generative modelling we generally seek to learn lower-dimensional representations rather than higher dimensional ones. This is because trying to learn a representation of higher dimension that the observation is a fruitless endeavor without strong priors. <span style="color:rgb(146, 208, 80)"><span style="color:rgb(192, 0, 0)">On the other hand learning lower-dimension latents can also be seen as a form of compression and can potentially uncover semantically meaningful structure describing observations.</span></span> Ì¥

> Problem of Generative Modelling
> The problem of generative modelling in most cases is posed as _parametric density estimation_ using finite set of samples $\{ x^n\}_{n=1}^N$ froma true but unknown data distribution $q_{data}(x)$. With a suitable model family chosen as $p_{\theta}(x)$ with unknown parameters $\theta$, the problem boils down to maximizing the average log-likelihood (w.r.t $\theta$) of all the samples under the model.

$$
\theta^* = \operatorname*{argmax}_{\theta} \mathbb{E}_{x\sim p_{data}(x)}[log\ p_{\theta}(x)] \approx \operatorname*{argmax}_{\theta} \frac{1}{N}\sum\limits_{i=1}^Nlogp_{\theta}(x^i)
$$

It turns out that defining arbitrary parametric density $p_{\theta}(x)$ is not as easy as it looks. There was one aspect of $p_{\theta}(x)$ that is widely considered to be the evil behind this difficulty - the normalizing constant that stems from the axiom of probability.
$$
p_{\theta}(x) = \frac{\tilde{p_{\theta}(x)}}{\int_x\tilde{p_{\theta}(x)}}
$$

It was understood quite early that any promising generative model must have atleast one property - ease of sampling, i.e. generating new samples. Sampling was so essential to generative modelling that the model families that followed were all geared towards effective sampling, even if it was at the expense of not so important properties. It was also well understood that there was one common underlying principle most effective for crafting "sampling-centric" generative models - transforming simple probability densities. This formed the backbone of every single generative model so far, their generative process is a density transformation of this form
$$
x = f_\theta(z)\ where\ z \sim \mathcal{N(0, I)}
$$
that suggests start with a simple density (often just standard normal) followed by functional transformation $f_\theta$ , typically a neural network with parameters $\theta$.

