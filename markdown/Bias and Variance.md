In general as we increase tunable parameters in a model, it becomes more flexible and can better fit a training data set. it is said to have lower error or bias.

However, for more flexible models, there will tend  to be greater variance to the model fit each time we take a set of samples to create a new training set. It is said **there is greater variance in in model's estimated parameters**