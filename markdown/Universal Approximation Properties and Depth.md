
- Universal Approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any squashing "activation" function can approximate any Borel measureable function from on finite dimensional space to another with any desired nonzero amount of error provided that the network is given enough hidden units.But we dont know if the training algorithm will be able to learn the function. Learning can fail for two different reasons
	- First the optimization alogrithm used for training may not be able to find the value of the parameters that corresponds to desired function.
	- Secondly, the training algorithm might choose the wrong function as a result of overfitting.
- Feedforward Networks provide a universal system for representing functions in the sense that, given a function, there exists a feedforward network that approximates the function. There is no universal procedure for examining a training set of specific examples and choosing a function that will generalize to points not in the training set.
- FeedForward networks with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error
- We may also want to choose a deep model for statistical reasons. Any time we choose a specific machine learning algorithm, we are implicitly stating some set of **prior beliefs** we have about what kind of function the algorithm should learn. Choosing a deep model encodes a very general belief that the functions we want to learn should involve composition of several simpler functions. 